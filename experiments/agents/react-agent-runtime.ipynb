{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc23ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install z3-solver langgraph langchain-openai ipywidgets requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Literal, Optional, Any\n",
    "from enum import IntEnum\n",
    "import requests\n",
    "from z3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c9add",
   "metadata": {},
   "source": [
    "## Part 1: Permission Model (mirrors ACL2 spec)\n",
    "\n",
    "File access and execute are orthogonal permissions:\n",
    "- File access: none (0), read (1), read-write (2)\n",
    "- Execute: separate boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ceec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessLevel(IntEnum):\n",
    "    \"\"\"File access levels - matches ACL2 *access-none*, *access-read*, *access-read-write*\"\"\"\n",
    "    NONE = 0\n",
    "    READ = 1\n",
    "    READ_WRITE = 2\n",
    "\n",
    "def access_sufficient(required: int, granted: int) -> bool:\n",
    "    \"\"\"Check if granted access >= required access (mirrors ACL2 access-sufficient-p)\"\"\"\n",
    "    return granted >= required\n",
    "\n",
    "def tool_permitted(required_access: int, requires_execute: bool, \n",
    "                   granted_access: int, execute_allowed: bool) -> bool:\n",
    "    \"\"\"Check if tool can be invoked (mirrors ACL2 tool-permitted-p)\"\"\"\n",
    "    access_ok = access_sufficient(required_access, granted_access)\n",
    "    execute_ok = (not requires_execute) or execute_allowed\n",
    "    return access_ok and execute_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3951d09",
   "metadata": {},
   "source": [
    "## Part 2: LLM Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b5b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LLMModelSpec:\n",
    "    \"\"\"LLM model specification (mirrors ACL2 llm-model-spec-p)\"\"\"\n",
    "    name: str\n",
    "    tokens_per_second: float  # throughput for time estimation\n",
    "    cost_per_1k_input: float  # cost in millicents per 1000 input tokens\n",
    "    cost_per_1k_output: float # cost in millicents per 1000 output tokens\n",
    "    \n",
    "    def call_cost(self, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Calculate LLM call cost in millicents (mirrors ACL2 llm-call-cost)\"\"\"\n",
    "        return ((input_tokens / 1000) * self.cost_per_1k_input + \n",
    "                (output_tokens / 1000) * self.cost_per_1k_output)\n",
    "    \n",
    "    def call_time_ms(self, input_tokens: int, output_tokens: int) -> float:\n",
    "        \"\"\"Calculate LLM call time in milliseconds (mirrors ACL2 llm-call-time-ms)\"\"\"\n",
    "        return 1000 * (input_tokens + output_tokens) / self.tokens_per_second\n",
    "\n",
    "# Manual model registry (to be populated from LM Studio)\n",
    "MODEL_REGISTRY: dict[str, LLMModelSpec] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d598cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_lm_studio_models(base_url: str = \"http://host.docker.internal:1234\") -> list[str]:\n",
    "    \"\"\"Fetch available models from LM Studio\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{base_url}/v1/models\", timeout=5)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        models = [m[\"id\"] for m in data.get(\"data\", [])]\n",
    "        print(f\"Found {len(models)} models: {models}\")\n",
    "        return models\n",
    "    except Exception as e:\n",
    "        print(f\"Could not connect to LM Studio: {e}\")\n",
    "        return []\n",
    "\n",
    "def register_model(name: str, tps: float = 50.0, \n",
    "                   cost_in: float = 0.0, cost_out: float = 0.0):\n",
    "    \"\"\"Register a model with performance characteristics\"\"\"\n",
    "    MODEL_REGISTRY[name] = LLMModelSpec(name, tps, cost_in, cost_out)\n",
    "    print(f\"Registered model: {name} ({tps} tok/s)\")\n",
    "\n",
    "# Fetch and register models\n",
    "available_models = fetch_lm_studio_models()\n",
    "for model in available_models:\n",
    "    # Default: 50 tok/s, free (local)\n",
    "    register_model(model, tps=50.0, cost_in=0.0, cost_out=0.0)\n",
    "\n",
    "# Fallback default model if LM Studio not available\n",
    "if not MODEL_REGISTRY:\n",
    "    register_model(\"default-local\", tps=50.0, cost_in=0.0, cost_out=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c4357",
   "metadata": {},
   "source": [
    "## Part 3: Tool Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4134d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ToolSpec:\n",
    "    \"\"\"Tool specification (mirrors ACL2 tool-spec-p)\"\"\"\n",
    "    name: str\n",
    "    required_access: AccessLevel\n",
    "    requires_execute: bool\n",
    "    base_cost: int          # millicents\n",
    "    time_estimate_ms: int   # milliseconds\n",
    "    token_estimate: int     # tokens added to context\n",
    "\n",
    "# Example tool registry\n",
    "TOOL_REGISTRY: dict[str, ToolSpec] = {\n",
    "    \"read_file\": ToolSpec(\"read_file\", AccessLevel.READ, False, 0, 100, 500),\n",
    "    \"write_file\": ToolSpec(\"write_file\", AccessLevel.READ_WRITE, False, 0, 200, 100),\n",
    "    \"run_python\": ToolSpec(\"run_python\", AccessLevel.NONE, True, 0, 5000, 200),\n",
    "    \"web_search\": ToolSpec(\"web_search\", AccessLevel.NONE, False, 10, 2000, 1000),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a6021d",
   "metadata": {},
   "source": [
    "## Part 4: Agent State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Agent state (mirrors ACL2 agent-state-p)\"\"\"\n",
    "    iteration: int = 0\n",
    "    max_iterations: int = 10\n",
    "    token_budget: int = 100000      # remaining tokens\n",
    "    cost_budget: int = 100000       # remaining millicents ($1.00)\n",
    "    time_budget_ms: int = 60000     # remaining time (60 seconds)\n",
    "    file_access: AccessLevel = AccessLevel.READ\n",
    "    execute_allowed: bool = False\n",
    "    satisfaction: float = 0.0       # 0.0 to 1.0\n",
    "    done: bool = False\n",
    "    \n",
    "    # Messages for LangGraph\n",
    "    messages: list = field(default_factory=list)\n",
    "    \n",
    "    def copy(self) -> 'AgentState':\n",
    "        \"\"\"Create a copy of the state\"\"\"\n",
    "        return AgentState(\n",
    "            iteration=self.iteration,\n",
    "            max_iterations=self.max_iterations,\n",
    "            token_budget=self.token_budget,\n",
    "            cost_budget=self.cost_budget,\n",
    "            time_budget_ms=self.time_budget_ms,\n",
    "            file_access=self.file_access,\n",
    "            execute_allowed=self.execute_allowed,\n",
    "            satisfaction=self.satisfaction,\n",
    "            done=self.done,\n",
    "            messages=self.messages.copy()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c82ca",
   "metadata": {},
   "source": [
    "## Part 5: Z3 Constraint Checking\n",
    "\n",
    "This is where Z3 enforces the constraints proven in ACL2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20983d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants matching ACL2 spec\n",
    "MIN_LLM_TOKENS = 100\n",
    "MIN_ITERATION_COST = 10\n",
    "MIN_ITERATION_TIME = 1000\n",
    "SATISFACTION_THRESHOLD = 0.9\n",
    "\n",
    "def z3_must_respond(state: AgentState) -> bool:\n",
    "    \"\"\"Check if agent must respond now (mirrors ACL2 must-respond-p)\n",
    "    \n",
    "    Uses Z3 to verify the constraint.\n",
    "    \"\"\"\n",
    "    s = Solver()\n",
    "    \n",
    "    # Z3 variables for state\n",
    "    done = Bool('done')\n",
    "    iteration = Int('iteration')\n",
    "    max_iter = Int('max_iter')\n",
    "    token_budget = Int('token_budget')\n",
    "    cost_budget = Int('cost_budget')\n",
    "    time_budget = Int('time_budget')\n",
    "    \n",
    "    # Add current state as constraints\n",
    "    s.add(done == state.done)\n",
    "    s.add(iteration == state.iteration)\n",
    "    s.add(max_iter == state.max_iterations)\n",
    "    s.add(token_budget == state.token_budget)\n",
    "    s.add(cost_budget == state.cost_budget)\n",
    "    s.add(time_budget == state.time_budget_ms)\n",
    "    \n",
    "    # Must respond condition (mirrors ACL2)\n",
    "    must_respond = Or(\n",
    "        done,\n",
    "        iteration >= max_iter,\n",
    "        token_budget < MIN_LLM_TOKENS,\n",
    "        cost_budget < MIN_ITERATION_COST,\n",
    "        time_budget < MIN_ITERATION_TIME\n",
    "    )\n",
    "    \n",
    "    # Check if must_respond is satisfiable (it always is, we want its value)\n",
    "    s.add(must_respond)\n",
    "    return s.check() == sat\n",
    "\n",
    "def z3_should_continue(state: AgentState) -> bool:\n",
    "    \"\"\"Check if agent should continue (mirrors ACL2 should-continue-p)\n",
    "    \n",
    "    Uses Z3 to verify: NOT must_respond AND satisfaction < threshold\n",
    "    \"\"\"\n",
    "    if z3_must_respond(state):\n",
    "        return False\n",
    "    return state.satisfaction < SATISFACTION_THRESHOLD\n",
    "\n",
    "def z3_can_invoke_tool(state: AgentState, tool: ToolSpec) -> bool:\n",
    "    \"\"\"Check if tool can be invoked (mirrors ACL2 can-invoke-tool-p)\n",
    "    \n",
    "    Verifies both permission and budget constraints via Z3.\n",
    "    \"\"\"\n",
    "    s = Solver()\n",
    "    \n",
    "    # Permission check\n",
    "    required_access = Int('required_access')\n",
    "    requires_execute = Bool('requires_execute')\n",
    "    granted_access = Int('granted_access')\n",
    "    execute_allowed = Bool('execute_allowed')\n",
    "    \n",
    "    s.add(required_access == tool.required_access.value)\n",
    "    s.add(requires_execute == tool.requires_execute)\n",
    "    s.add(granted_access == state.file_access.value)\n",
    "    s.add(execute_allowed == state.execute_allowed)\n",
    "    \n",
    "    # Permission constraint (mirrors ACL2 tool-permitted-p)\n",
    "    access_ok = granted_access >= required_access\n",
    "    execute_ok = Or(Not(requires_execute), execute_allowed)\n",
    "    permission_ok = And(access_ok, execute_ok)\n",
    "    \n",
    "    # Budget check\n",
    "    tool_cost = Int('tool_cost')\n",
    "    tool_time = Int('tool_time')\n",
    "    tool_tokens = Int('tool_tokens')\n",
    "    cost_budget = Int('cost_budget')\n",
    "    time_budget = Int('time_budget')\n",
    "    token_budget = Int('token_budget')\n",
    "    \n",
    "    s.add(tool_cost == tool.base_cost)\n",
    "    s.add(tool_time == tool.time_estimate_ms)\n",
    "    s.add(tool_tokens == tool.token_estimate)\n",
    "    s.add(cost_budget == state.cost_budget)\n",
    "    s.add(time_budget == state.time_budget_ms)\n",
    "    s.add(token_budget == state.token_budget)\n",
    "    \n",
    "    # Budget constraint (mirrors ACL2 tool-budget-sufficient-p)\n",
    "    budget_ok = And(\n",
    "        tool_cost <= cost_budget,\n",
    "        tool_time <= time_budget,\n",
    "        tool_tokens <= token_budget\n",
    "    )\n",
    "    \n",
    "    # Both must hold\n",
    "    s.add(And(permission_ok, budget_ok))\n",
    "    \n",
    "    return s.check() == sat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d959c2",
   "metadata": {},
   "source": [
    "## Part 6: State Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduct_tool_cost(state: AgentState, tool: ToolSpec) -> AgentState:\n",
    "    \"\"\"Deduct tool costs from state (mirrors ACL2 deduct-tool-cost)\"\"\"\n",
    "    new_state = state.copy()\n",
    "    new_state.token_budget = max(0, state.token_budget - tool.token_estimate)\n",
    "    new_state.cost_budget = max(0, state.cost_budget - tool.base_cost)\n",
    "    new_state.time_budget_ms = max(0, state.time_budget_ms - tool.time_estimate_ms)\n",
    "    return new_state\n",
    "\n",
    "def deduct_llm_cost(state: AgentState, model: LLMModelSpec, \n",
    "                    input_tokens: int, output_tokens: int) -> AgentState:\n",
    "    \"\"\"Deduct LLM call costs from state\"\"\"\n",
    "    new_state = state.copy()\n",
    "    cost = model.call_cost(input_tokens, output_tokens)\n",
    "    time = model.call_time_ms(input_tokens, output_tokens)\n",
    "    \n",
    "    new_state.iteration += 1\n",
    "    new_state.token_budget = max(0, state.token_budget - (input_tokens + output_tokens))\n",
    "    new_state.cost_budget = max(0, int(state.cost_budget - cost))\n",
    "    new_state.time_budget_ms = max(0, int(state.time_budget_ms - time))\n",
    "    return new_state\n",
    "\n",
    "def update_satisfaction(state: AgentState, score: float) -> AgentState:\n",
    "    \"\"\"Update satisfaction score (mirrors ACL2 update-satisfaction)\"\"\"\n",
    "    new_state = state.copy()\n",
    "    new_state.satisfaction = max(0.0, min(1.0, score))\n",
    "    return new_state\n",
    "\n",
    "def mark_done(state: AgentState) -> AgentState:\n",
    "    \"\"\"Mark agent as done (mirrors ACL2 mark-done)\"\"\"\n",
    "    new_state = state.copy()\n",
    "    new_state.done = True\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf0617",
   "metadata": {},
   "source": [
    "## Part 7: UI Controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Permission controls\n",
    "read_permission = widgets.Checkbox(value=True, description='Read files')\n",
    "write_permission = widgets.Checkbox(value=False, description='Write files')\n",
    "execute_permission = widgets.Checkbox(value=False, description='Execute code')\n",
    "\n",
    "# Budget controls\n",
    "max_tokens = widgets.IntSlider(value=100000, min=1000, max=500000, step=1000, \n",
    "                                description='Max tokens:')\n",
    "max_cost = widgets.FloatSlider(value=1.0, min=0.01, max=10.0, step=0.01,\n",
    "                                description='Max cost ($):')\n",
    "max_time = widgets.IntSlider(value=60, min=5, max=300, step=5,\n",
    "                              description='Max time (s):')\n",
    "max_iterations = widgets.IntSlider(value=10, min=1, max=50, step=1,\n",
    "                                    description='Max iterations:')\n",
    "\n",
    "# Model selection\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=list(MODEL_REGISTRY.keys()) or ['default-local'],\n",
    "    description='LLM Model:'\n",
    ")\n",
    "\n",
    "# Display controls\n",
    "permissions_box = widgets.VBox([widgets.Label('Permissions:'), \n",
    "                                 read_permission, write_permission, execute_permission])\n",
    "budget_box = widgets.VBox([widgets.Label('Budgets:'),\n",
    "                           max_tokens, max_cost, max_time, max_iterations])\n",
    "model_box = widgets.VBox([widgets.Label('Model:'), model_dropdown])\n",
    "\n",
    "display(widgets.HBox([permissions_box, budget_box, model_box]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permissions_from_ui() -> tuple[AccessLevel, bool]:\n",
    "    \"\"\"Get current permissions from UI controls\"\"\"\n",
    "    if write_permission.value:\n",
    "        access = AccessLevel.READ_WRITE\n",
    "    elif read_permission.value:\n",
    "        access = AccessLevel.READ\n",
    "    else:\n",
    "        access = AccessLevel.NONE\n",
    "    return access, execute_permission.value\n",
    "\n",
    "def create_initial_state() -> AgentState:\n",
    "    \"\"\"Create initial agent state from UI controls\"\"\"\n",
    "    access, execute = get_permissions_from_ui()\n",
    "    return AgentState(\n",
    "        iteration=0,\n",
    "        max_iterations=max_iterations.value,\n",
    "        token_budget=max_tokens.value,\n",
    "        cost_budget=int(max_cost.value * 100000),  # dollars to millicents\n",
    "        time_budget_ms=max_time.value * 1000,       # seconds to ms\n",
    "        file_access=access,\n",
    "        execute_allowed=execute,\n",
    "        satisfaction=0.0,\n",
    "        done=False,\n",
    "        messages=[]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d879488",
   "metadata": {},
   "source": [
    "## Part 8: LLM-as-Judge for Satisfaction Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901cccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# LM Studio connection\n",
    "LM_STUDIO_BASE_URL = \"http://host.docker.internal:1234/v1\"\n",
    "\n",
    "def get_llm(model_name: str = None) -> ChatOpenAI:\n",
    "    \"\"\"Get LLM client configured for LM Studio\"\"\"\n",
    "    model = model_name or model_dropdown.value\n",
    "    return ChatOpenAI(\n",
    "        base_url=LM_STUDIO_BASE_URL,\n",
    "        api_key=\"lm-studio\",  # LM Studio doesn't require a real key\n",
    "        model=model,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"You are evaluating how well the current response answers the original question.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Current response/progress:\n",
    "{response}\n",
    "\n",
    "Rate the satisfaction on a scale of 0-10, where:\n",
    "- 0: No progress toward answering\n",
    "- 5: Partial answer, significant gaps\n",
    "- 10: Complete, accurate answer\n",
    "\n",
    "Respond with ONLY a single number 0-10.\"\"\"\n",
    "\n",
    "def assess_satisfaction(question: str, response: str, llm: ChatOpenAI = None) -> float:\n",
    "    \"\"\"Use LLM-as-judge to assess response satisfaction (0.0 to 1.0)\"\"\"\n",
    "    if llm is None:\n",
    "        llm = get_llm()\n",
    "    \n",
    "    try:\n",
    "        prompt = JUDGE_PROMPT.format(question=question, response=response)\n",
    "        result = llm.invoke([HumanMessage(content=prompt)])\n",
    "        score = int(result.content.strip())\n",
    "        return max(0.0, min(1.0, score / 10.0))\n",
    "    except Exception as e:\n",
    "        print(f\"Satisfaction assessment failed: {e}\")\n",
    "        return 0.5  # default middle score on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c80224",
   "metadata": {},
   "source": [
    "## Part 9: LangGraph Agent with Z3 Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390cf815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"State for LangGraph\"\"\"\n",
    "    messages: Annotated[list, operator.add]\n",
    "    agent_state: AgentState\n",
    "    question: str\n",
    "\n",
    "def call_llm(state: GraphState) -> GraphState:\n",
    "    \"\"\"Call LLM and update state\"\"\"\n",
    "    llm = get_llm()\n",
    "    agent_st = state[\"agent_state\"]\n",
    "    \n",
    "    # Estimate tokens (rough)\n",
    "    input_tokens = sum(len(m.content) // 4 for m in state[\"messages\"])\n",
    "    expected_output = 500\n",
    "    \n",
    "    # Get model spec\n",
    "    model_spec = MODEL_REGISTRY.get(model_dropdown.value)\n",
    "    if model_spec is None:\n",
    "        model_spec = LLMModelSpec(\"default\", 50.0, 0.0, 0.0)\n",
    "    \n",
    "    # Call LLM\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    actual_output = len(response.content) // 4\n",
    "    \n",
    "    # Update state with costs\n",
    "    new_agent_st = deduct_llm_cost(agent_st, model_spec, input_tokens, actual_output)\n",
    "    \n",
    "    # Assess satisfaction\n",
    "    satisfaction = assess_satisfaction(\n",
    "        state[\"question\"], \n",
    "        response.content,\n",
    "        llm\n",
    "    )\n",
    "    new_agent_st = update_satisfaction(new_agent_st, satisfaction)\n",
    "    \n",
    "    print(f\"Iteration {new_agent_st.iteration}: satisfaction={satisfaction:.2f}, \"\n",
    "          f\"tokens_left={new_agent_st.token_budget}, \"\n",
    "          f\"time_left={new_agent_st.time_budget_ms}ms\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"agent_state\": new_agent_st,\n",
    "        \"question\": state[\"question\"]\n",
    "    }\n",
    "\n",
    "def should_continue(state: GraphState) -> Literal[\"continue\", \"end\"]:\n",
    "    \"\"\"Z3-based routing decision (mirrors ACL2 should-continue-p)\"\"\"\n",
    "    agent_st = state[\"agent_state\"]\n",
    "    \n",
    "    if z3_should_continue(agent_st):\n",
    "        print(f\"  -> Z3: should continue (satisfaction={agent_st.satisfaction:.2f} < {SATISFACTION_THRESHOLD})\")\n",
    "        return \"continue\"\n",
    "    else:\n",
    "        reason = \"done\" if agent_st.done else \\\n",
    "                 \"max iterations\" if agent_st.iteration >= agent_st.max_iterations else \\\n",
    "                 \"satisfaction met\" if agent_st.satisfaction >= SATISFACTION_THRESHOLD else \\\n",
    "                 \"budget exhausted\"\n",
    "        print(f\"  -> Z3: must respond ({reason})\")\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent_graph() -> StateGraph:\n",
    "    \"\"\"Build the LangGraph agent with Z3 routing\"\"\"\n",
    "    graph = StateGraph(GraphState)\n",
    "    \n",
    "    # Add nodes\n",
    "    graph.add_node(\"llm\", call_llm)\n",
    "    \n",
    "    # Add edges\n",
    "    graph.set_entry_point(\"llm\")\n",
    "    graph.add_conditional_edges(\n",
    "        \"llm\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"llm\",\n",
    "            \"end\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return graph.compile()\n",
    "\n",
    "agent = build_agent_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2154b",
   "metadata": {},
   "source": [
    "## Part 10: Run the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(question: str) -> str:\n",
    "    \"\"\"Run the verified ReAct agent on a question\"\"\"\n",
    "    initial_state = create_initial_state()\n",
    "    \n",
    "    print(f\"Starting agent with:\")\n",
    "    print(f\"  - File access: {initial_state.file_access.name}\")\n",
    "    print(f\"  - Execute allowed: {initial_state.execute_allowed}\")\n",
    "    print(f\"  - Token budget: {initial_state.token_budget}\")\n",
    "    print(f\"  - Cost budget: ${initial_state.cost_budget / 100000:.2f}\")\n",
    "    print(f\"  - Time budget: {initial_state.time_budget_ms / 1000}s\")\n",
    "    print(f\"  - Max iterations: {initial_state.max_iterations}\")\n",
    "    print()\n",
    "    \n",
    "    graph_state: GraphState = {\n",
    "        \"messages\": [\n",
    "            SystemMessage(content=\"You are a helpful assistant. Think step by step.\"),\n",
    "            HumanMessage(content=question)\n",
    "        ],\n",
    "        \"agent_state\": initial_state,\n",
    "        \"question\": question\n",
    "    }\n",
    "    \n",
    "    result = agent.invoke(graph_state)\n",
    "    \n",
    "    final_state = result[\"agent_state\"]\n",
    "    print(f\"\\nFinal state:\")\n",
    "    print(f\"  - Iterations used: {final_state.iteration}\")\n",
    "    print(f\"  - Final satisfaction: {final_state.satisfaction:.2f}\")\n",
    "    print(f\"  - Tokens remaining: {final_state.token_budget}\")\n",
    "    \n",
    "    # Return last AI message\n",
    "    for msg in reversed(result[\"messages\"]):\n",
    "        if isinstance(msg, AIMessage):\n",
    "            return msg.content\n",
    "    return \"No response generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f373000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the agent\n",
    "# Adjust the UI controls above before running\n",
    "\n",
    "question = \"What is the capital of France and what is it known for?\"\n",
    "response = run_agent(question)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41399737",
   "metadata": {},
   "source": [
    "## Verification Notes\n",
    "\n",
    "This notebook implements the same constraints proven in `experiment-01-react-verified.lisp`:\n",
    "\n",
    "1. **Permission Safety** (`permission-safety` theorem): `z3_can_invoke_tool` ensures tool permissions are satisfied\n",
    "2. **Budget Non-negativity** (`tool-deduction-preserves-budget-nonneg`): `deduct_tool_cost` uses `max(0, ...)` \n",
    "3. **Iteration Increases** (`iteration-increases`): `deduct_llm_cost` increments iteration\n",
    "4. **Termination** (`termination-by-iteration`): `z3_must_respond` checks iteration bound\n",
    "\n",
    "The Z3 solver provides runtime enforcement of the constraints proven correct in ACL2."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
