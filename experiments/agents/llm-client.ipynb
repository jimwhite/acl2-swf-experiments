{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
       "; LLM Client -- HTTP client for LLM API calls\n",
       ";\n",
       "; Copyright (C) 2025\n",
       ";\n",
       "; License: See LICENSE file\n",
       ";\n",
       "; Author: AI Assistant with human guidance\n",
       ";\n",
       "; This book provides a verified LLM client using properly-guarded HTTP JSON\n",
       "; functions. All guards are maintained for formal verification."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(in-package \"ACL2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(include-book \"llm-types\")\n",
       "(include-book \"http-json\" \n",
       "              :ttags ((:quicklisp) (:quicklisp.dexador) (:http-json)))\n",
       "(include-book \"std/strings/explode-nonnegative-integer\" :dir :system)\n",
       "(include-book \"kestrel/json-parser/parse-json\" :dir :system)\n",
       "; (depends-on \"llm-client-raw.lsp\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
       ";; Configuration\n",
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defconst *lm-studio-endpoint* \n",
       "  \"http://host.docker.internal:1234/v1/chat/completions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; OpenAI-compatible models endpoint (basic info only)\n",
       "(defconst *lm-studio-models-endpoint*\n",
       "  \"http://host.docker.internal:1234/v1/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; LM Studio native API (full model info with type, state, context length)\n",
       "(defconst *lm-studio-v0-models-endpoint*\n",
       "  \"http://host.docker.internal:1234/api/v0/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defconst *llm-connect-timeout* 30)   ; seconds\n",
       "(defconst *llm-read-timeout* 120)     ; seconds (higher for slow local models)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
       ";; JSON Serialization Helpers\n",
       ";;\n",
       ";; Logical specifications - raw Lisp provides actual implementation.\n",
       ";; These maintain proper guards (stringp returns).\n",
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Serialize a single chat message to JSON object string\n",
       ";; Input: chat-message-p\n",
       ";; Output: stringp like {\"role\":\"user\",\"content\":\"hello\"}\n",
       "(defun serialize-chat-message (msg)\n",
       "  (declare (xargs :guard (chat-message-p msg))\n",
       "           (ignore msg))\n",
       "  (prog2$ (er hard? 'serialize-chat-message \"Raw Lisp definition not installed?\")\n",
       "          \"{}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm stringp-of-serialize-chat-message\n",
       "  (stringp (serialize-chat-message msg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Serialize a list of chat messages to JSON array string\n",
       ";; Input: chat-message-list-p\n",
       ";; Output: stringp like [{\"role\":\"user\",\"content\":\"hello\"}]\n",
       "(defun serialize-chat-messages (messages)\n",
       "  (declare (xargs :guard (chat-message-list-p messages))\n",
       "           (ignore messages))\n",
       "  (prog2$ (er hard? 'serialize-chat-messages \"Raw Lisp definition not installed?\")\n",
       "          \"[]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm stringp-of-serialize-chat-messages\n",
       "  (stringp (serialize-chat-messages messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Serialize full chat completion request to JSON string\n",
       ";; Input: model (stringp), messages (chat-message-list-p)\n",
       ";; Output: stringp like {\"model\":\"...\",\"messages\":[...]}\n",
       "(defun serialize-chat-request (model messages)\n",
       "  (declare (xargs :guard (and (stringp model) (chat-message-list-p messages)))\n",
       "           (ignore model messages))\n",
       "  (prog2$ (er hard? 'serialize-chat-request \"Raw Lisp definition not installed?\")\n",
       "          \"{}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm stringp-of-serialize-chat-request\n",
       "  (stringp (serialize-chat-request model messages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Parse chat completion response JSON, extract assistant message content\n",
       ";; Input: json response string\n",
       ";; Output: content string (empty on parse failure)\n",
       ";; Note: Implementation in llm-client-raw.lsp uses kestrel/json-parser\n",
       "(defun parse-chat-response (json)\n",
       "  (declare (xargs :guard (stringp json))\n",
       "           (ignore json))\n",
       "  (prog2$ (er hard? 'parse-chat-response \"Raw Lisp definition not installed?\")\n",
       "          \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm stringp-of-parse-chat-response\n",
       "  (stringp (parse-chat-response json)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Parse models response JSON, extract list of model IDs\n",
       ";; Input: json response string from /v1/models (OpenAI format)\n",
       ";; Output: list of model ID strings (nil on parse failure)\n",
       ";; Note: Implementation in llm-client-raw.lsp uses kestrel/json-parser\n",
       "(defun parse-models-response (json)\n",
       "  (declare (xargs :guard (stringp json))\n",
       "           (ignore json))\n",
       "  (prog2$ (er hard? 'parse-models-response \"Raw Lisp definition not installed?\")\n",
       "          nil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm string-listp-of-parse-models-response\n",
       "  (string-listp (parse-models-response json)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Parse LM Studio v0 models response JSON, extract full model info\n",
       ";; Input: json response string from /api/v0/models \n",
       ";; Output: list of model-info-p (nil on parse failure)\n",
       ";; Note: Implementation in llm-client-raw.lsp uses kestrel/json-parser\n",
       "(defun parse-v0-models-response (json)\n",
       "  (declare (xargs :guard (stringp json))\n",
       "           (ignore json))\n",
       "  (prog2$ (er hard? 'parse-v0-models-response \"Raw Lisp definition not installed?\")\n",
       "          nil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm model-info-list-p-of-parse-v0-models-response\n",
       "  (model-info-list-p (parse-v0-models-response json)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
       ";; Main API\n",
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Helper to check if HTTP status indicates success (2xx)\n",
       "(defun http-success-p (status)\n",
       "  (declare (xargs :guard (natp status)))\n",
       "  (and (>= status 200)\n",
       "       (< status 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm booleanp-of-http-success-p\n",
       "  (booleanp (http-success-p status))\n",
       "  :rule-classes :type-prescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Call LLM chat completion API\n",
       ";;\n",
       ";; Parameters:\n",
       ";;   model    - Model identifier string (e.g., \"local-model\")\n",
       ";;   messages - Conversation history as chat-message-list\n",
       ";;   state    - ACL2 state\n",
       ";;\n",
       ";; Returns: (mv error response state)\n",
       ";;   error    - NIL on success, error string on failure\n",
       ";;   response - Assistant's response content (stringp, empty on error)\n",
       ";;   state    - Updated state\n",
       "(defun llm-chat-completion (model messages state)\n",
       "  (declare (xargs :guard (and (stringp model)\n",
       "                              (chat-message-list-p messages))\n",
       "                  :stobjs state\n",
       "                  :guard-hints ((\"Goal\" :in-theory (disable post-json)))))\n",
       "  (b* (;; Serialize the request to JSON\n",
       "       (request-json (serialize-chat-request model messages))\n",
       "       \n",
       "       ;; HTTP headers for JSON API\n",
       "       (headers '((\"Content-Type\" . \"application/json\")\n",
       "                  (\"Accept\" . \"application/json\")))\n",
       "       \n",
       "       ;; Make HTTP POST request with proper guards\n",
       "       ((mv err response-body status-raw state)\n",
       "        (post-json *lm-studio-endpoint*\n",
       "                   request-json\n",
       "                   headers\n",
       "                   *llm-connect-timeout*\n",
       "                   *llm-read-timeout*\n",
       "                   state))\n",
       "       \n",
       "       ;; Coerce status to natp (it is, via theorem, but help guard verification)\n",
       "       (status (mbe :logic (nfix status-raw) :exec status-raw))\n",
       "       \n",
       "       ;; Check for network/connection errors\n",
       "       ((when err)\n",
       "        (mv err \"\" state))\n",
       "       \n",
       "       ;; Check for HTTP error status\n",
       "       ((unless (http-success-p status))\n",
       "        (mv (concatenate 'string \"HTTP error: status \" \n",
       "                        (coerce (explode-nonnegative-integer status 10 nil) 'string))\n",
       "            \"\"\n",
       "            state))\n",
       "       \n",
       "       ;; Parse the response JSON to extract assistant content\n",
       "       (content (parse-chat-response response-body)))\n",
       "    \n",
       "    (mv nil content state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Return type theorems for llm-chat-completion\n",
       "(defthm stringp-of-llm-chat-completion-response\n",
       "  (stringp (mv-nth 1 (llm-chat-completion model messages state))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm state-p1-of-llm-chat-completion\n",
       "  (implies (state-p1 state)\n",
       "           (state-p1 (mv-nth 2 (llm-chat-completion model messages state)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; List available models from LLM server\n",
       ";;\n",
       ";; Parameters:\n",
       ";;   state - ACL2 state\n",
       ";;\n",
       ";; Returns: (mv error models state)\n",
       ";;   error  - NIL on success, error string on failure\n",
       ";;   models - List of model ID strings (string-listp)\n",
       ";;   state  - Updated state\n",
       "(defun llm-list-models (state)\n",
       "  (declare (xargs :stobjs state))\n",
       "  (b* (;; HTTP headers for JSON API\n",
       "       (headers '((\"Accept\" . \"application/json\")))\n",
       "       \n",
       "       ;; Make HTTP GET request\n",
       "       ((mv err response-body status-raw state)\n",
       "        (get-json *lm-studio-models-endpoint*\n",
       "                  headers\n",
       "                  *llm-connect-timeout*\n",
       "                  *llm-read-timeout*\n",
       "                  state))\n",
       "       \n",
       "       ;; Coerce status to natp\n",
       "       (status (mbe :logic (nfix status-raw) :exec status-raw))\n",
       "       \n",
       "       ;; Check for network/connection errors\n",
       "       ((when err)\n",
       "        (mv err nil state))\n",
       "       \n",
       "       ;; Check for HTTP error status\n",
       "       ((unless (http-success-p status))\n",
       "        (mv (concatenate 'string \"HTTP error: status \" \n",
       "                        (coerce (explode-nonnegative-integer status 10 nil) 'string))\n",
       "            nil\n",
       "            state))\n",
       "       \n",
       "       ;; Parse the response JSON to extract model list\n",
       "       (models (parse-models-response response-body)))\n",
       "    \n",
       "    (mv nil models state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Return type theorems for llm-list-models\n",
       "(defthm string-listp-of-llm-list-models-models\n",
       "  (string-listp (mv-nth 1 (llm-list-models state))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm state-p1-of-llm-list-models\n",
       "  (implies (state-p1 state)\n",
       "           (state-p1 (mv-nth 2 (llm-list-models state)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; List available models with full info from LM Studio v0 API\n",
       ";;\n",
       ";; Parameters:\n",
       ";;   state - ACL2 state\n",
       ";;\n",
       ";; Returns: (mv error models state)\n",
       ";;   error  - NIL on success, error string on failure\n",
       ";;   models - List of model-info-p with full details\n",
       ";;   state  - Updated state\n",
       "(defun llm-list-models-full (state)\n",
       "  (declare (xargs :stobjs state))\n",
       "  (b* (;; HTTP headers for JSON API\n",
       "       (headers '((\"Accept\" . \"application/json\")))\n",
       "       \n",
       "       ;; Make HTTP GET request to v0 API\n",
       "       ((mv err response-body status-raw state)\n",
       "        (get-json *lm-studio-v0-models-endpoint*\n",
       "                  headers\n",
       "                  *llm-connect-timeout*\n",
       "                  *llm-read-timeout*\n",
       "                  state))\n",
       "       \n",
       "       ;; Coerce status to natp\n",
       "       (status (mbe :logic (nfix status-raw) :exec status-raw))\n",
       "       \n",
       "       ;; Check for network/connection errors\n",
       "       ((when err)\n",
       "        (mv err nil state))\n",
       "       \n",
       "       ;; Check for HTTP error status\n",
       "       ((unless (http-success-p status))\n",
       "        (mv (concatenate 'string \"HTTP error: status \" \n",
       "                        (coerce (explode-nonnegative-integer status 10 nil) 'string))\n",
       "            nil\n",
       "            state))\n",
       "       \n",
       "       ;; Parse the response JSON to extract full model info\n",
       "       (models (parse-v0-models-response response-body)))\n",
       "    \n",
       "    (mv nil models state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Return type theorems for llm-list-models-full\n",
       "(defthm model-info-list-p-of-llm-list-models-full-models\n",
       "  (model-info-list-p (mv-nth 1 (llm-list-models-full state))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defthm state-p1-of-llm-list-models-full\n",
       "  (implies (state-p1 state)\n",
       "           (state-p1 (mv-nth 2 (llm-list-models-full state)))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
       ";; Model Selection\n",
       ";;\n",
       ";; Select the best model from available models based on preferences.\n",
       ";; Default: first loaded completions model that matches any preference string.\n",
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Check if model ID contains a preference substring (case-insensitive would be \n",
       ";; nice but we'll do exact substring for now)\n",
       "(defun model-matches-pref-p (model-id pref)\n",
       "  (declare (xargs :guard (and (stringp model-id) (stringp pref))))\n",
       "  (search pref model-id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Find first model matching any of the preferences\n",
       ";; Implementation in raw Lisp uses LOOP for efficiency\n",
       "(defun find-matching-model (models prefs)\n",
       "  (declare (xargs :guard (and (model-info-list-p models)\n",
       "                              (string-listp prefs)))\n",
       "           (ignore models prefs))\n",
       "  (prog2$ (er hard? 'find-matching-model \"Raw Lisp definition not installed?\")\n",
       "          nil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       ";; Select best model for completions\n",
       ";;\n",
       ";; Parameters:\n",
       ";;   models - Full model info list from llm-list-models-full\n",
       ";;   prefs  - List of preference strings to match (partial match on model ID)\n",
       ";;\n",
       ";; Returns: model-info-p or nil if no suitable model found\n",
       ";;\n",
       ";; Selection order:\n",
       ";; 1. First loaded completions model matching a preference (in pref order)\n",
       ";; 2. First loaded completions model (if no prefs or no match)\n",
       ";; 3. NIL if no loaded completions models\n",
       "(defun select-completions-model (models prefs)\n",
       "  (declare (xargs :guard (and (model-info-list-p models)\n",
       "                              (string-listp prefs)))\n",
       "           (ignore models prefs))\n",
       "  (prog2$ (er hard? 'select-completions-model \"Raw Lisp definition not installed?\")\n",
       "          nil))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
       ";; Trust tag and raw Lisp inclusion for serialization functions\n",
       ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "acl2"
    }
   },
   "outputs": [],
   "source": [
       "(defttag :llm-client)\n",
       "(include-raw \"llm-client-raw.lsp\"\n",
       "             :host-readtable t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACL2",
   "language": "acl2",
   "name": "acl2"
  },
  "language_info": {
   "name": "acl2",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
